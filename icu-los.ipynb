{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af9f1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id   stay_id  seq_num icd_code  icd_version  \\\n",
      "0    10000032  32952584        1     4589            9   \n",
      "1    10000032  32952584        2    07070            9   \n",
      "2    10000032  32952584        3      V08            9   \n",
      "3    10000032  39399961        1    78097            9   \n",
      "4    10000032  39399961        2    34830            9   \n",
      "\n",
      "                                           icd_title icd_combined  \n",
      "0                                    HYPOTENSION NOS       9-4589  \n",
      "1  UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC ...      9-07070  \n",
      "2                         ASYMPTOMATIC HIV INFECTION        9-V08  \n",
      "3                             ALTERED MENTAL STATUS       9-78097  \n",
      "4                        ENCEPHALOPATHY, UNSPECIFIED      9-34830  \n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "icu_data = pd.read_csv('icu/icustays_cleaned.csv')\n",
    "diagnosis = pd.read_csv('ed/diagnosis_cleaned.csv')\n",
    "triage = pd.read_csv('ed/triage_cleaned.csv')\n",
    "vitals = pd.read_csv('ed/vitals_cleaned.csv')\n",
    "ed_stays = pd.read_csv('ed/edstays_cleaned.csv')\n",
    "\n",
    "# Combine the icd_version and icd_code columns in diagnosis table\n",
    "diagnosis['icd_combined'] = diagnosis['icd_version'].astype(str) + '-' + diagnosis['icd_code'].astype(str)\n",
    "print(diagnosis.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d414eb",
   "metadata": {},
   "source": [
    "Following code links emergency department (ED) stays with ICU admissions and their corresponding length of stay (los). The resulting DataFrame is essential for downstream tasks like predictive modeling or analysis of ICU utilization based on ED visit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4189bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stay_id     hadm_id       los\n",
      "0  32952584  29079034.0  0.410266\n",
      "1  39399961  29079034.0  0.410266\n",
      "2  30905710  26913865.0  0.497535\n",
      "3  39866888  24597018.0  1.118032\n",
      "4  34719194  26184834.0  9.171817\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "merged_ed_icu = ed_stays[['stay_id', 'hadm_id']].merge(icu_data[['hadm_id', 'los']], on='hadm_id', how='inner')\n",
    "merged_ed_icu = merged_ed_icu[['stay_id', 'hadm_id', 'los']]\n",
    "\n",
    "print(merged_ed_icu.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3cb60",
   "metadata": {},
   "source": [
    "#### Processing Vitals Data\n",
    "In this section, we process the `vitals` data to extract meaningful features that capture both static and dynamic aspects of a patient’s condition during their stay in the emergency department (ED). This approach ensures that the data is prepared for downstream tasks like predictive modeling or analysis. \n",
    "\n",
    "#### Why Process Vitals Data This Way?\n",
    "- Static Aggregates for Overall Trends: The vital signs data (`heartrate`, `resprate`, `o2sat`, `sbp`, `dbp`) often includes multiple time-stamped measurements for each `stay_id`. Aggregating these values into a single `mean` value provides a concise summary of the patient's overall condition during their ED stay.\n",
    "- Dynamic Trends for Detailed Insights: Aggregates like mean values can overlook significant temporal trends or changes in a patient’s condition. By calculating the slope of each vital sign over time (using linear regression), we capture these trends, such as:\n",
    "    - Improvement: A patient’s oxygen saturation (o2sat) increases over time.\n",
    "    - Deterioration: A patient’s blood pressure (sbp, dbp) decreases over time.\n",
    "    \n",
    "    These trends are critical for understanding how a patient’s condition evolves and can be strong predictors for downstream analysis (e.g., ICU length of stay).\n",
    "- Combining Aggregates and Trends: Merging the static (mean, represente as _x) and dynamic (slope, represente as _slope) features ensures that both high-level and fine-grained patterns in the data are included for analysis or modeling.\n",
    "\n",
    "#### Key Steps in the Code:\n",
    "1. Aggregate Vitals Data: The `groupby` operation on `stay_id` aggregates vital sign measurements for each patient stay, providing static features like mean values for each vital sign.\n",
    "\n",
    "2. Calculate Slope for Dynamic Trends: The `calculate_slope` function calculates the rate of change (slope) for each vital sign over time by performing linear regression on the time-stamped data (`charttime`) and the vital sign values.\n",
    "\n",
    "3. Merge Vitals and Triage Data: Combines the `vitals_summary` data (static features) with the `triage` data using a left join on `stay_id`. `_x` refers to columns from the `triage` dataset. `_y` refers to columns from the `vitals_summary` dataset, which is the `mean` value.\n",
    "\n",
    "4. Merge Static and Dynamic Features: After aggregating static values and calculating dynamic slopes, dynamic (slope) features are merged with other data to create a comprehensive feature set. \n",
    "\n",
    "5. Retain Desired Columns for Further Use: Only the most relevant columns (static aggregates, slopes, and identifiers) are retained for use in downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec1a5",
   "metadata": {},
   "source": [
    "I think these can be write in the report.\n",
    "\n",
    "Benefits of This Approach:\n",
    "- Comprehensive Data Representation: Combining static and dynamic features captures both the overall condition and temporal patterns of a patient’s vitals.\n",
    "- Improved Predictive Power: Trends (slopes) can highlight critical insights that static features alone cannot reveal, such as rapid deterioration or improvement in a patient’s condition.\n",
    "- Cleaner, More Organized Dataset: Aggregating and merging the vitals data ensures that the resulting dataset is ready for machine learning models or statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840b1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stay_id  heartrate_x  resprate_x  o2sat_x  sbp_x  dbp_x  heartrate_y  \\\n",
      "0  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "1  39399961         77.0        16.0     98.0   96.0   50.0    85.727273   \n",
      "2  30905710         80.0        25.0     97.0  132.0   83.0    70.000000   \n",
      "3  39866888         81.0        16.0     97.0  160.0  102.0    66.666667   \n",
      "4  34719194         80.0        24.0     98.0  116.0   66.0    84.666667   \n",
      "\n",
      "   resprate_y     o2sat_y       sbp_y      dbp_y  heartrate_slope  \\\n",
      "0   20.285714   98.000000   79.428571  42.857143         0.000133   \n",
      "1   16.909091   94.363636   84.181818  50.545455         0.000785   \n",
      "2   28.000000  100.000000  144.000000  87.000000        -0.001754   \n",
      "3   16.000000   99.000000  143.666667  97.333333         0.000210   \n",
      "4   21.166667   94.166667  135.600000  74.000000         0.000001   \n",
      "\n",
      "   resprate_slope  o2sat_slope  sbp_slope  dbp_slope  \n",
      "0       -0.000509     0.000099  -0.000282  -0.000404  \n",
      "1       -0.000081     0.000106   0.000016  -0.000728  \n",
      "2             NaN          NaN  -0.005263  -0.008772  \n",
      "3             NaN     0.000314   0.000628  -0.000210  \n",
      "4       -0.000270    -0.000177   0.000671   0.000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/6g9d3qsx78gflgy4qv_xd2th0000gn/T/ipykernel_34587/87066971.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  slope_data = vitals.groupby('stay_id').apply(\n"
     ]
    }
   ],
   "source": [
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "def calculate_slope(df, col):\n",
    "    df = df.sort_values('charttime').dropna(subset=[col])  # Drop NaN rows\n",
    "    time_delta = (pd.to_datetime(df['charttime'], format=date_format) - pd.to_datetime(df['charttime'], format=date_format).min()).dt.total_seconds()\n",
    "\n",
    "    if len(time_delta.unique()) < 2 or df[col].nunique() < 2:\n",
    "        return np.nan\n",
    "\n",
    "    slope = np.polyfit(time_delta, df[col], 1)[0]\n",
    "    return slope\n",
    "\n",
    "\n",
    "# Aggregate vitals data\n",
    "vitals_summary = vitals.groupby('stay_id').agg({\n",
    "    'heartrate': 'mean',\n",
    "    'resprate': 'mean',\n",
    "    'o2sat': 'mean',\n",
    "    'sbp': 'mean',\n",
    "    'dbp': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate slopes\n",
    "slope_data = vitals.groupby('stay_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'heartrate_slope': calculate_slope(x, 'heartrate'),\n",
    "        'resprate_slope': calculate_slope(x, 'resprate'),\n",
    "        'o2sat_slope': calculate_slope(x, 'o2sat'),\n",
    "        'sbp_slope': calculate_slope(x, 'sbp'),\n",
    "        'dbp_slope': calculate_slope(x, 'dbp')\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Merge triage and vitals summary\n",
    "merged_features = triage.merge(vitals_summary, on='stay_id', how='left')\n",
    "\n",
    "# Merge slopes into merged_features\n",
    "merged_features = merged_features.merge(slope_data, on='stay_id', how='left')\n",
    "\n",
    "# Retain only desired columns\n",
    "ed_features = merged_features[[\n",
    "    'stay_id', 'heartrate_x', 'resprate_x', 'o2sat_x', 'sbp_x', 'dbp_x',\n",
    "    'heartrate_y', 'resprate_y', 'o2sat_y', 'sbp_y', 'dbp_y', \n",
    "    'heartrate_slope', 'resprate_slope', 'o2sat_slope', 'sbp_slope', 'dbp_slope'\n",
    "]]\n",
    "\n",
    "# Verify result\n",
    "print(ed_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52935f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stay_id  heartrate_x  resprate_x  o2sat_x  sbp_x  dbp_x  heartrate_y  \\\n",
      "0  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "1  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "2  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "3  39399961         77.0        16.0     98.0   96.0   50.0    85.727273   \n",
      "4  39399961         77.0        16.0     98.0   96.0   50.0    85.727273   \n",
      "\n",
      "   resprate_y    o2sat_y      sbp_y  ...  heartrate_slope  resprate_slope  \\\n",
      "0   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "1   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "2   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "3   16.909091  94.363636  84.181818  ...         0.000785       -0.000081   \n",
      "4   16.909091  94.363636  84.181818  ...         0.000785       -0.000081   \n",
      "\n",
      "   o2sat_slope  sbp_slope  dbp_slope     hadm_id       los  icd_combined  \\\n",
      "0     0.000099  -0.000282  -0.000404  29079034.0  0.410266        9-4589   \n",
      "1     0.000099  -0.000282  -0.000404  29079034.0  0.410266       9-07070   \n",
      "2     0.000099  -0.000282  -0.000404  29079034.0  0.410266         9-V08   \n",
      "3     0.000106   0.000016  -0.000728  29079034.0  0.410266       9-78097   \n",
      "4     0.000106   0.000016  -0.000728  29079034.0  0.410266       9-34830   \n",
      "\n",
      "                intime              outtime  \n",
      "0  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "1  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "2  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "3  2180-07-23 05:54:00  2180-07-23 14:00:00  \n",
      "4  2180-07-23 05:54:00  2180-07-23 14:00:00  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#Merge ED features with ICU and Diagnosis Data\n",
    "final_data = pd.merge(ed_features, merged_ed_icu[['stay_id', 'hadm_id', 'los']], on='stay_id', how='inner')\n",
    "final_data = pd.merge(final_data, diagnosis[['stay_id', 'icd_combined']], on='stay_id', how='left')\n",
    "final_data = pd.merge(final_data, ed_stays[['stay_id', 'intime', 'outtime']], on='stay_id', how='left')\n",
    "\n",
    "# Verify result\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee405c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "def impute_missing_values(df, numeric_features, non_numeric_features):\n",
    "    # Impute numeric columns using the mean\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_features] = numeric_imputer.fit_transform(df[numeric_features])\n",
    "    \n",
    "    # Impute non-numeric columns with a constant value, e.g., 'unknown'\n",
    "    non_numeric_imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "    df[non_numeric_features] = non_numeric_imputer.fit_transform(df[non_numeric_features])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define numeric and non-numeric features for imputation\n",
    "numeric_features = ['heartrate_x', 'resprate_x', 'o2sat_x', 'sbp_x', 'dbp_x',\n",
    "                    'heartrate_y', 'resprate_y', 'o2sat_y', 'sbp_y', 'dbp_y', \n",
    "                    'heartrate_slope', 'resprate_slope', 'o2sat_slope', 'sbp_slope', 'dbp_slope'\n",
    "                    ]\n",
    "non_numeric_features = ['icd_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84f5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stay_id  heartrate_x  resprate_x  o2sat_x  sbp_x  dbp_x  heartrate_y  \\\n",
      "0  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "1  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "2  32952584         87.0        14.0     97.0   71.0   43.0    84.571429   \n",
      "3  39399961         77.0        16.0     98.0   96.0   50.0    85.727273   \n",
      "4  39399961         77.0        16.0     98.0   96.0   50.0    85.727273   \n",
      "\n",
      "   resprate_y    o2sat_y      sbp_y  ...  heartrate_slope  resprate_slope  \\\n",
      "0   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "1   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "2   20.285714  98.000000  79.428571  ...         0.000133       -0.000509   \n",
      "3   16.909091  94.363636  84.181818  ...         0.000785       -0.000081   \n",
      "4   16.909091  94.363636  84.181818  ...         0.000785       -0.000081   \n",
      "\n",
      "   o2sat_slope  sbp_slope  dbp_slope     hadm_id       los  icd_combined  \\\n",
      "0     0.000099  -0.000282  -0.000404  29079034.0  0.410266        9-4589   \n",
      "1     0.000099  -0.000282  -0.000404  29079034.0  0.410266       9-07070   \n",
      "2     0.000099  -0.000282  -0.000404  29079034.0  0.410266         9-V08   \n",
      "3     0.000106   0.000016  -0.000728  29079034.0  0.410266       9-78097   \n",
      "4     0.000106   0.000016  -0.000728  29079034.0  0.410266       9-34830   \n",
      "\n",
      "                intime              outtime  \n",
      "0  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "1  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "2  2180-07-22 16:24:00  2180-07-23 05:54:00  \n",
      "3  2180-07-23 05:54:00  2180-07-23 14:00:00  \n",
      "4  2180-07-23 05:54:00  2180-07-23 14:00:00  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the imputation function\n",
    "final_data = impute_missing_values(final_data, numeric_features, non_numeric_features)\n",
    "\n",
    "# Function to calculate and handle outliers\n",
    "def remove_outliers(df, col_ranges):\n",
    "    for col_prefix, (min_val, max_val) in col_ranges.items():\n",
    "        # Handle both '_x' and '_y' columns for each vital sign\n",
    "        for suffix in ['_x', '_y']:\n",
    "            col = col_prefix + suffix\n",
    "            if col in df.columns:\n",
    "                df[col] = np.where(df[col] < min_val, min_val, df[col])\n",
    "                df[col] = np.where(df[col] > max_val, max_val, df[col])\n",
    "    return df\n",
    "\n",
    "# Define column ranges for vital signs\n",
    "col_ranges = {\n",
    "    'heartrate': (30, 180),\n",
    "    'resprate': (8, 60),\n",
    "    'o2sat': (50, 100),\n",
    "    'sbp': (60, 250),\n",
    "    'dbp': (30, 150),\n",
    "}\n",
    "\n",
    "# Apply outlier removal\n",
    "final_data = remove_outliers(final_data, col_ranges)\n",
    "\n",
    "# Verify result\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c4ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed data saved to 'preprocessed_data.csv'\n",
      "(75509, 20)\n"
     ]
    }
   ],
   "source": [
    "# Prepare ED los data for normalization\n",
    "final_data['intime'] = pd.to_datetime(final_data['intime'])\n",
    "final_data['outtime'] = pd.to_datetime(final_data['outtime'])\n",
    "final_data['ed_los'] = (final_data['outtime'] - final_data['intime']).dt.total_seconds() / 3600 # convert to hours\n",
    "\n",
    "# Normalize data\n",
    "id_columns = ['stay_id', 'hadm_id', 'icd_combined', 'los']\n",
    "normalize_cols = ['heartrate_x', 'resprate_x', 'o2sat_x', 'sbp_x', 'dbp_x', \n",
    "                  'heartrate_y', 'resprate_y', 'o2sat_y', 'sbp_y', 'dbp_y', \n",
    "                  'heartrate_slope', 'resprate_slope', 'o2sat_slope', 'sbp_slope', 'dbp_slope', 'ed_los']\n",
    "\n",
    "# Separate data into two parts: identifiers and features to normalize\n",
    "id_data = final_data[id_columns].copy()\n",
    "features_to_normalize = final_data[normalize_cols].copy()\n",
    "\n",
    "# Handle missing values in features before normalization\n",
    "features_to_normalize.fillna(features_to_normalize.mean(), inplace=True)\n",
    "\n",
    "# Normalize only the numeric columns\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features_to_normalize)\n",
    "normalized_features_df = pd.DataFrame(normalized_features, columns=normalize_cols)\n",
    "\n",
    "# Concatenate the identifier columns with the normalized features\n",
    "final_data_normalized = pd.concat([id_data.reset_index(drop=True), normalized_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Handle missing values in non-numeric columns\n",
    "final_data_normalized['icd_combined'] = final_data_normalized['icd_combined'].fillna('unknown')\n",
    "\n",
    "# Save the processed data\n",
    "final_data_normalized.to_csv('preprocessed_data.csv', index=False)\n",
    "print(\"Final preprocessed data saved to 'preprocessed_data.csv'\")\n",
    "\n",
    "# Size of final_data_normalized\n",
    "print(final_data_normalized.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
